"""
Veri Ã‡ekme ve Ã–nbellekleme ModÃ¼lÃ¼
=================================
Bu modÃ¼l Yahoo Finance'den hisse senedi verilerini Ã§eker,
Ã¶nbelleÄŸe alÄ±r ve eksik gÃ¼nleri doldurur.

Ã–nemli: SPY (S&P 500 ETF) benchmark olarak her zaman otomatik Ã§ekilir.
"""

import os
import time
import pandas as pd
import yfinance as yf
from datetime import datetime, timedelta
from typing import List, Tuple, Optional
from dataclasses import dataclass

# Cache klasÃ¶rÃ¼nÃ¼n yolu (proje kÃ¶k dizinine gÃ¶re)
CACHE_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), "data", "cache")

# Benchmark sembolÃ¼ - her zaman otomatik Ã§ekilecek
BENCHMARK_SYMBOL = "SPY"


@dataclass
class DataResult:
    """Veri Ã§ekme sonucu - demo data kullanÄ±ldÄ±ÄŸÄ±nda flag dÃ¶ner."""
    stock_prices: pd.DataFrame
    benchmark_prices: Optional[pd.DataFrame]
    is_demo_data: bool = False  # demo/synthetic veri mi?
    failed_tickers: List[str] = None  # Ã§ekilemeyen ticker'lar
    
    def __post_init__(self):
        if self.failed_tickers is None:
            self.failed_tickers = []



def _ensure_cache_dir():
    """
    Cache klasÃ¶rÃ¼ yoksa oluÅŸturur.
    Ä°lk Ã§alÄ±ÅŸtÄ±rmada klasÃ¶r olmayabilir diye kontrol ediyoruz.
    """
    if not os.path.exists(CACHE_DIR):
        os.makedirs(CACHE_DIR)


def _exponential_backoff(func, max_retries: int = 5, base_delay: float = 1.0):
    """
    Rate limit durumunda Ã¼stel geri Ã§ekilme stratejisi.
    
    NasÄ±l Ã§alÄ±ÅŸÄ±r:
    - Ä°lk hata: 1 saniye bekle
    - Ä°kinci hata: 2 saniye bekle
    - ÃœÃ§Ã¼ncÃ¼ hata: 4 saniye bekle
    - vs...
    
    Args:
        func: Ã‡alÄ±ÅŸtÄ±rÄ±lacak fonksiyon
        max_retries: Maksimum deneme sayÄ±sÄ±
        base_delay: BaÅŸlangÄ±Ã§ bekleme sÃ¼resi (saniye)
    
    Returns:
        Fonksiyonun sonucu
    """
    last_exception = None
    
    for attempt in range(max_retries):
        try:
            return func()
        except Exception as e:
            last_exception = e
            if attempt < max_retries - 1:
                # her denemede bekleme sÃ¼resini 2 katÄ±na Ã§Ä±kar
                wait_time = base_delay * (2 ** attempt)
                print(f"Hata oluÅŸtu, {wait_time:.1f} saniye bekleniyor... (Deneme {attempt + 1}/{max_retries})")
                time.sleep(wait_time)
    
    # tÃ¼m denemeler baÅŸarÄ±sÄ±z olduysa son hatayÄ± fÄ±rlat
    raise last_exception


def _get_cache_filename(ticker: str, start_date: str, end_date: str) -> str:
    """
    Cache dosyasÄ± iÃ§in benzersiz isim oluÅŸturur.
    Ã–rnek: AAPL_2023-01-01_2024-01-01.csv
    """
    # tarih formatÄ±nÄ± dÃ¼zelt (/ yerine - kullan)
    start_clean = start_date.replace("/", "-")
    end_clean = end_date.replace("/", "-")
    return f"{ticker}_{start_clean}_{end_clean}.csv"


def save_to_cache(df: pd.DataFrame, ticker: str, start_date: str, end_date: str) -> str:
    """
    DataFrame'i CSV olarak cache klasÃ¶rÃ¼ne kaydeder.
    
    Args:
        df: Kaydedilecek veri
        ticker: Hisse senedi sembolÃ¼
        start_date: BaÅŸlangÄ±Ã§ tarihi
        end_date: BitiÅŸ tarihi
    
    Returns:
        Kaydedilen dosyanÄ±n yolu
    """
    _ensure_cache_dir()
    filename = _get_cache_filename(ticker, start_date, end_date)
    filepath = os.path.join(CACHE_DIR, filename)
    df.to_csv(filepath)
    print(f"âœ“ {ticker} verisi Ã¶nbelleÄŸe kaydedildi: {filename}")
    return filepath


@dataclass
class CacheAnalysis:
    """Cache analiz sonucu - eksik veri bilgisini tutar."""
    data: Optional[pd.DataFrame]  # Cache'den okunan veri
    coverage: float               # Kapsama oranÄ± (0-1)
    cache_start: Optional[datetime] = None
    cache_end: Optional[datetime] = None
    missing_start: Optional[datetime] = None  # Eksik baÅŸlangÄ±Ã§ tarihi
    missing_end: Optional[datetime] = None    # Eksik bitiÅŸ tarihi
    missing_days: int = 0                     # Eksik gÃ¼n sayÄ±sÄ±
    is_complete: bool = False                 # Tam kapsama var mÄ±?


def analyze_cache(ticker: str, start_date: str, end_date: str) -> CacheAnalysis:
    """
    Cache durumunu analiz eder ve eksik veri bilgisini dÃ¶ndÃ¼rÃ¼r.
    
    Bu fonksiyon:
    - Cache'de ne kadar veri olduÄŸunu kontrol eder
    - Eksik tarih aralÄ±ÄŸÄ±nÄ± tespit eder
    - Kapsama oranÄ±nÄ± hesaplar
    
    Args:
        ticker: Hisse senedi sembolÃ¼
        start_date: Ä°stenen baÅŸlangÄ±Ã§ tarihi
        end_date: Ä°stenen bitiÅŸ tarihi
    
    Returns:
        CacheAnalysis objesi
    """
    _ensure_cache_dir()
    
    requested_start = pd.to_datetime(start_date)
    requested_end = pd.to_datetime(end_date)
    expected_days = len(pd.bdate_range(requested_start, requested_end))
    
    # BoÅŸ sonuÃ§
    empty_result = CacheAnalysis(
        data=None, coverage=0.0, missing_start=requested_start,
        missing_end=requested_end, missing_days=expected_days
    )
    
    if expected_days == 0:
        return empty_result
    
    # En iyi cache adayÄ±nÄ± bul
    best_df = None
    best_coverage = 0.0
    best_cache_start = None
    best_cache_end = None
    
    try:
        cache_files = [f for f in os.listdir(CACHE_DIR) 
                       if f.startswith(f"{ticker}_") and f.endswith(".csv")]
    except Exception:
        return empty_result
    
    for fname in cache_files:
        fpath = os.path.join(CACHE_DIR, fname)
        
        try:
            df = pd.read_csv(fpath, index_col=0, parse_dates=True)
            df.index = pd.to_datetime(df.index)
            
            if len(df) == 0:
                continue
            
            cache_start = df.index.min()
            cache_end = df.index.max()
            
            # Ä°stenen aralÄ±kla kesiÅŸen veriyi al
            filtered = df[(df.index >= requested_start) & (df.index <= requested_end)]
            if len(filtered) == 0:
                continue
            
            coverage = len(filtered) / expected_days
            
            if coverage > best_coverage:
                best_coverage = coverage
                best_df = filtered
                best_cache_start = cache_start
                best_cache_end = cache_end
                
        except Exception:
            continue
    
    if best_df is None:
        return empty_result
    
    # Eksik tarih aralÄ±ÄŸÄ±nÄ± hesapla
    missing_start = None
    missing_end = None
    missing_days = 0
    
    if best_coverage < 1.0:
        # BaÅŸta mÄ± eksik?
        if best_cache_start > requested_start:
            missing_start = requested_start
            missing_end = best_cache_start - timedelta(days=1)
            missing_days += len(pd.bdate_range(missing_start, missing_end))
        
        # Sonda mÄ± eksik?
        if best_cache_end < requested_end:
            if missing_start is None:
                missing_start = best_cache_end + timedelta(days=1)
            missing_end = requested_end
            missing_days += len(pd.bdate_range(best_cache_end + timedelta(days=1), requested_end))
    
    return CacheAnalysis(
        data=best_df,
        coverage=best_coverage,
        cache_start=best_cache_start,
        cache_end=best_cache_end,
        missing_start=missing_start,
        missing_end=missing_end,
        missing_days=missing_days,
        is_complete=(best_coverage >= 0.9999)
    )


def load_from_cache(ticker: str, start_date: str, end_date: str, 
                    allow_partial: bool = True) -> Optional[pd.DataFrame]:
    """
    Cache'den veri okumaya Ã§alÄ±ÅŸÄ±r.
    
    AKILLI CACHE STRATEJÄ°SÄ°:
    ========================
    1. TAM EÅLEÅME: AynÄ± tarih aralÄ±ÄŸÄ± iÃ§in daha Ã¶nce Ã§ekilmiÅŸ veri varsa kullan
    2. TAM KAPSAMA: Daha geniÅŸ aralÄ±klÄ± cache varsa, isteneni filtrele
    3. KISMÄ° KAPSAMA (%90+): allow_partial=True ise kabul et
    
    Args:
        ticker: Hisse senedi sembolÃ¼
        start_date: BaÅŸlangÄ±Ã§ tarihi (YYYY-MM-DD)
        end_date: BitiÅŸ tarihi (YYYY-MM-DD)
        allow_partial: %90+ kÄ±smi kapsamayÄ± kabul et (varsayÄ±lan: True)
    
    Returns:
        DataFrame veya None (cache yetersizse)
    """
    analysis = analyze_cache(ticker, start_date, end_date)
    
    if analysis.data is None:
        return None
    
    # Tam kapsama
    if analysis.is_complete:
        print(f"âœ“ {ticker} cache tam kapsÄ±yor ({len(analysis.data)} gÃ¼n)")
        return analysis.data
    
    # KÄ±smi kapsama
    if analysis.coverage >= 0.90:
        if allow_partial:
            print(f"âœ“ {ticker} cache %{analysis.coverage*100:.0f} kapsÄ±yor ({len(analysis.data)} gÃ¼n)")
            if analysis.missing_days > 0:
                print(f"  â„¹ï¸ Eksik: {analysis.missing_start.strftime('%Y-%m-%d') if analysis.missing_start else '?'} â†’ "
                      f"{analysis.missing_end.strftime('%Y-%m-%d') if analysis.missing_end else '?'} ({analysis.missing_days} iÅŸ gÃ¼nÃ¼)")
            return analysis.data
        else:
            print(f"â„¹ï¸ {ticker} cache %{analysis.coverage*100:.0f} kapsÄ±yor ama tam veri istendi")
            return None
    
    # Yetersiz kapsama
    if analysis.coverage > 0:
        print(f"â„¹ï¸ {ticker} cache sadece %{analysis.coverage*100:.0f} kapsÄ±yor, canlÄ± veri denenecek")
    
    return None


def _fetch_live_data(ticker: str, start_date: str, end_date: str) -> Tuple[Optional[pd.DataFrame], str]:
    """
    Yahoo Finance'den canlÄ± veri Ã§eker (internal helper).
    
    Args:
        ticker: Hisse senedi sembolÃ¼
        start_date: BaÅŸlangÄ±Ã§ tarihi
        end_date: BitiÅŸ tarihi
    
    Returns:
        Tuple: (DataFrame veya None, kaynak)
    """
    try:
        print(f"â³ {ticker} canlÄ± veri indiriliyor ({start_date} â†’ {end_date})...")
        
        def _download():
            data = yf.download(ticker, start=start_date, end=end_date, progress=False)
            if data.empty:
                raise ValueError(f"{ticker} iÃ§in veri bulunamadÄ±!")
            return data
        
        data = _exponential_backoff(_download, max_retries=3, base_delay=1.0)
        
        # yfinance v1.0: 'Adj Close' kaldÄ±rÄ±ldÄ±, artÄ±k 'Close' kullanÄ±lÄ±yor
        if isinstance(data.columns, pd.MultiIndex):
            if 'Close' in data.columns.get_level_values(0):
                close_prices = data['Close']
            elif 'Adj Close' in data.columns.get_level_values(0):
                close_prices = data['Adj Close']
            else:
                close_prices = data.iloc[:, 0]
        else:
            if 'Close' in data.columns:
                close_prices = data[['Close']]
            elif 'Adj Close' in data.columns:
                close_prices = data[['Adj Close']]
            else:
                close_prices = data.iloc[:, :1]
        
        # tek kolon varsa DataFrame olarak tut
        if isinstance(close_prices, pd.Series):
            close_prices = close_prices.to_frame(name=ticker)
        else:
            close_prices.columns = [ticker]
        
        if len(close_prices) > 0:
            print(f"âœ“ {ticker} canlÄ± veri alÄ±ndÄ±: {len(close_prices)} gÃ¼n")
            return close_prices, "live"
        
        return None, "empty"
        
    except Exception as e:
        print(f"âš ï¸ {ticker} canlÄ± veri hatasÄ±: {e}")
        return None, "error"


def _filter_date_range(df: pd.DataFrame, start_date: str, end_date: str) -> pd.DataFrame:
    """
    DataFrame'i belirtilen tarih aralÄ±ÄŸÄ±na filtreler.
    Demo verileri daha geniÅŸ bir aralÄ±k iÃ§erebilir.
    """
    # index'in datetime oldugunu garanti et
    df.index = pd.to_datetime(df.index)
    
    start = pd.to_datetime(start_date)
    end = pd.to_datetime(end_date)
    
    # tarih araligi filtrele
    mask = (df.index >= start) & (df.index <= end)
    filtered = df.loc[mask]
    
    if len(filtered) == 0:
        print(f"âš ï¸ Belirtilen tarih aralÄ±ÄŸÄ±nda veri yok, tÃ¼m veri kullanÄ±lÄ±yor")
        return df
    
    return filtered


def load_csv_fallback(filepath: str) -> pd.DataFrame:
    """
    KullanÄ±cÄ±nÄ±n manuel olarak yÃ¼klediÄŸi CSV dosyasÄ±nÄ± okur.
    Bu fonksiyon API Ã§alÄ±ÅŸmadÄ±ÄŸÄ±nda yedek olarak kullanÄ±lÄ±r.
    
    Args:
        filepath: CSV dosyasÄ±nÄ±n tam yolu
    
    Returns:
        DataFrame (index=tarih, kolonlar=hisse sembolleri)
    """
    df = pd.read_csv(filepath, index_col=0, parse_dates=True)
    print(f"âœ“ CSV dosyasÄ± yÃ¼klendi: {filepath}")
    return df


def fetch_single_stock(
    ticker: str, 
    start_date: str, 
    end_date: str,
    force_live: bool = False,      # False = cache Ã¶ncelikli (varsayÄ±lan)
    force_complete: bool = False   # True = eksik veriyi tamamla
) -> Tuple[pd.DataFrame, str]:
    """
    Tek bir hisse senedi iÃ§in veri Ã§eker.
    
    VERÄ° Ã‡EKME STRATEJÄ°SÄ°:
    ======================
    1. Cache'e bak (akÄ±llÄ± cache - tam/kÄ±smi kapsama kontrolÃ¼)
    2. Cache yetersizse â†’ CanlÄ± veri Ã§ek (yfinance)
    3. CanlÄ± da baÅŸarÄ±sÄ±zsa â†’ Cache fallback (varsa)
    4. HiÃ§biri olmazsa â†’ Sentetik veri Ã¼ret (demo amaÃ§lÄ±)
    
    YENÄ°: INCREMENTAL FETCH (force_complete=True)
    =============================================
    - Cache'de eksik olan kÄ±smÄ± tespit et
    - SADECE eksik tarihleri Ã§ek (API tasarrufu)
    - Eski ve yeni veriyi birleÅŸtir
    - GÃ¼ncellenmiÅŸ cache'i kaydet
    
    Args:
        ticker: Hisse senedi sembolÃ¼ (Ã¶rn: "AAPL")
        start_date: BaÅŸlangÄ±Ã§ tarihi (YYYY-MM-DD)
        end_date: BitiÅŸ tarihi (YYYY-MM-DD)
        force_live: True ise cache'i atla, direkt canlÄ± dene
        force_complete: True ise eksik veriyi tamamla (incremental fetch)
    
    Returns:
        Tuple: (fiyat_df, veri_kaynagi)
        veri_kaynagi: "live", "cache", "cache+live" veya "synthetic"
    """
    data_source = "unknown"
    close_prices = None
    
    # 1. CACHE ANALÄ°ZÄ°
    if not force_live:
        analysis = analyze_cache(ticker, start_date, end_date)
        
        # Tam kapsama - direkt dÃ¶n
        if analysis.is_complete and analysis.data is not None:
            return analysis.data, "cache"
        
        # KÄ±smi kapsama
        if analysis.data is not None and analysis.coverage >= 0.90:
            # KullanÄ±cÄ± tam veri istemiyorsa, kÄ±smi kabul et
            if not force_complete:
                print(f"âœ“ {ticker} cache %{analysis.coverage*100:.0f} kapsÄ±yor ({len(analysis.data)} gÃ¼n)")
                if analysis.missing_days > 0:
                    print(f"  â„¹ï¸ Eksik: {analysis.missing_days} iÅŸ gÃ¼nÃ¼ "
                          f"({analysis.missing_end.strftime('%Y-%m-%d') if analysis.missing_end else '?'} tarihine kadar)")
                return analysis.data, "cache"
            
            # INCREMENTAL FETCH: Sadece eksik kÄ±smÄ± Ã§ek
            if analysis.missing_end is not None and analysis.missing_days > 0:
                print(f"ğŸ”„ {ticker} iÃ§in eksik {analysis.missing_days} gÃ¼n Ã§ekiliyor...")
                
                # Eksik kÄ±sÄ±m sonda ise (en yaygÄ±n durum)
                if analysis.cache_end and analysis.cache_end < pd.to_datetime(end_date):
                    try:
                        # Sadece eksik tarihleri Ã§ek
                        incremental_start = (analysis.cache_end + timedelta(days=1)).strftime('%Y-%m-%d')
                        incremental_data, inc_source = _fetch_live_data(ticker, incremental_start, end_date)
                        
                        if incremental_data is not None and len(incremental_data) > 0:
                            # Eski ve yeni veriyi birleÅŸtir
                            combined = pd.concat([analysis.data, incremental_data])
                            combined = combined[~combined.index.duplicated(keep='last')]
                            combined = combined.sort_index()
                            
                            # GÃ¼ncellenmiÅŸ cache'i kaydet
                            save_to_cache(combined, ticker, start_date, end_date)
                            
                            print(f"âœ“ {ticker} gÃ¼ncellendi: {len(analysis.data)} + {len(incremental_data)} = {len(combined)} gÃ¼n")
                            return combined, "cache+live"
                    except Exception as e:
                        print(f"âš ï¸ {ticker} incremental fetch baÅŸarÄ±sÄ±z: {e}")
                        # Hata durumunda mevcut cache'i kullan
                        return analysis.data, "cache"
        
        # %90'Ä±n altÄ±nda ama bir miktar veri var - fallback iÃ§in sakla
        cached_fallback = analysis.data if analysis.data is not None else None
    else:
        cached_fallback = None
    
    # 2. TAM CANLI VERÄ° Ã‡EK
    try:
        print(f"â³ {ticker} canlÄ± veri indiriliyor...")
        
        def _download():
            data = yf.download(ticker, start=start_date, end=end_date, progress=False)
            if data.empty:
                raise ValueError(f"{ticker} iÃ§in veri bulunamadÄ±!")
            return data
        
        data = _exponential_backoff(_download, max_retries=3, base_delay=1.0)
        
        # yfinance v1.0: 'Adj Close' kaldÄ±rÄ±ldÄ±, artÄ±k 'Close' kullanÄ±lÄ±yor
        # SÃ¼tunlar MultiIndex formatÄ±nda: ('Close', 'AAPL')
        if isinstance(data.columns, pd.MultiIndex):
            # MultiIndex durumu - ('Close', ticker) formatÄ±nda
            if 'Close' in data.columns.get_level_values(0):
                close_prices = data['Close']
            elif 'Adj Close' in data.columns.get_level_values(0):
                close_prices = data['Adj Close']
            else:
                # Ä°lk sayÄ±sal kolonu al
                close_prices = data.iloc[:, 0]
        else:
            # DÃ¼z kolonlar
            if 'Close' in data.columns:
                close_prices = data[['Close']]
            elif 'Adj Close' in data.columns:
                close_prices = data[['Adj Close']]
            else:
                close_prices = data.iloc[:, :1]
        
        # tek kolon varsa DataFrame olarak tut
        if isinstance(close_prices, pd.Series):
            close_prices = close_prices.to_frame(name=ticker)
        else:
            close_prices.columns = [ticker]
        
        # Tarih aralÄ±ÄŸÄ± validasyonu
        if len(close_prices) > 0:
            actual_start = close_prices.index.min()
            actual_end = close_prices.index.max()
            requested_start = pd.to_datetime(start_date)
            requested_end = pd.to_datetime(end_date)
            
            # Minimum veri kontrolÃ¼
            expected_days = len(pd.bdate_range(requested_start, requested_end))
            actual_days = len(close_prices)
            coverage = actual_days / expected_days if expected_days > 0 else 0
            
            if coverage >= 0.50:  # En az %50 veri varsa kabul et
                data_source = "live"
                
                if coverage < 0.90:
                    print(f"âš ï¸ {ticker} kÄ±smi veri alÄ±ndÄ±: {actual_days}/{expected_days} gÃ¼n (%{coverage*100:.0f})")
                else:
                    print(f"âœ“ {ticker} canlÄ± veri alÄ±ndÄ±: {actual_days} gÃ¼n")
                
                # BaÅŸarÄ±lÄ± canlÄ± veriyi cache'e kaydet
                save_to_cache(close_prices, ticker, start_date, end_date)
                return close_prices, data_source
            else:
                print(f"âš ï¸ {ticker} canlÄ± veri yetersiz: sadece {actual_days} gÃ¼n ({coverage*100:.0f}%)")
        else:
            print(f"âš ï¸ {ticker} canlÄ± veri boÅŸ")
        
    except Exception as e:
        print(f"âš ï¸ {ticker} canlÄ± veri hatasÄ±: {e}")
    
    # 3. CACHE'E BAK (canlÄ± baÅŸarÄ±sÄ±z olursa fallback)
    # Not: force_live=True durumunda cache henÃ¼z kontrol edilmedi
    if force_live:
        cached = load_from_cache(ticker, start_date, end_date)
        if cached is not None and len(cached) > 0:
            data_source = "cache"
            print(f"â„¹ï¸ {ticker} cache'den yÃ¼klendi (fallback): {len(cached)} gÃ¼n")
            return cached, data_source
    
    # 4. SENTETÄ°K VERÄ° ÃœRET (son Ã§are - sadece demo amaÃ§lÄ±!)
    try:
        print(f"ğŸ”§ {ticker} iÃ§in sentetik veri Ã¼retiliyor (DEMO - gerÃ§ek veri deÄŸil!)...")
        
        # scripts klasÃ¶rÃ¼nden import
        import sys
        project_root = os.path.dirname(os.path.dirname(__file__))
        scripts_path = os.path.join(project_root, "scripts")
        if scripts_path not in sys.path:
            sys.path.insert(0, scripts_path)
        
        from generate_demo_data import generate_realistic_prices, STOCK_INFO
        
        # tarih araligi
        start = pd.to_datetime(start_date)
        end = pd.to_datetime(end_date)
        dates = pd.bdate_range(start, end)
        n_days = len(dates)
        
        if n_days == 0:
            raise ValueError(f"GeÃ§ersiz tarih aralÄ±ÄŸÄ±: {start_date} - {end_date}")
        
        # hisse bilgisi varsa kullan, yoksa varsayÄ±lan deÄŸerler
        if ticker in STOCK_INFO:
            info = STOCK_INFO[ticker]
        else:
            # varsayÄ±lan parametreler
            info = {"start_price": 100, "annual_return": 0.10, "annual_vol": 0.25}
            print(f"â„¹ï¸ {ticker} iÃ§in varsayÄ±lan parametreler kullanÄ±lÄ±yor")
        
        # fiyat serisi Ã¼ret
        prices = generate_realistic_prices(
            info["start_price"],
            info["annual_return"],
            info["annual_vol"],
            n_days,
            seed=hash(ticker) % 1000  # her ticker iÃ§in tutarlÄ± seed
        )
        
        close_prices = pd.DataFrame({ticker: prices}, index=dates)
        data_source = "synthetic"
        
        print(f"âš ï¸ {ticker} SENTETÄ°K veri Ã¼retildi: {len(close_prices)} gÃ¼n (GERÃ‡EK VERÄ° DEÄÄ°L!)")
        
        return close_prices, data_source
        
    except Exception as synth_error:
        print(f"âŒ {ticker} sentetik veri Ã¼retilemedi: {synth_error}")
        raise ValueError(
            f"{ticker} iÃ§in veri alÄ±namadÄ±! "
            f"CanlÄ±, cache veya sentetik veri Ã¼retilemedi."
        )


def fetch_stock_data(
    tickers: List[str],
    start_date: str,
    end_date: str,
    include_benchmark: bool = True,
    force_complete: bool = False  # YENÄ°: Eksik veriyi tamamla
) -> Tuple[pd.DataFrame, pd.DataFrame, dict]:
    """
    Birden fazla hisse senedi iÃ§in veri Ã§eker.
    
    Ã–NEMLÄ°: Benchmark (SPY) her zaman otomatik olarak Ã§ekilir!
    Bu sayede backtest aÅŸamasÄ±nda karÅŸÄ±laÅŸtÄ±rma yapabiliriz.
    
    Args:
        tickers: Hisse senedi sembolleri listesi (Ã¶rn: ["AAPL", "MSFT", "GOOGL"])
        start_date: BaÅŸlangÄ±Ã§ tarihi (YYYY-MM-DD)
        end_date: BitiÅŸ tarihi (YYYY-MM-DD)
        include_benchmark: Benchmark (SPY) dahil edilsin mi? (varsayÄ±lan: True)
        force_complete: Eksik veriyi tamamla (incremental fetch)
    
    Returns:
        Tuple: (hisse_fiyatlari, benchmark_fiyatlari, meta_bilgi)
        meta_bilgi: {"sources": {ticker: source}, "failed": [ticker_list], "missing_info": {...}}
    """
    all_prices = []
    data_sources = {}
    failed_tickers = []
    missing_info = {}  # Eksik veri bilgisi
    
    # kullanÄ±cÄ±nÄ±n seÃ§tiÄŸi hisseleri Ã§ek
    for ticker in tickers:
        ticker = ticker.strip().upper()  # boÅŸluklarÄ± temizle, bÃ¼yÃ¼k harfe Ã§evir
        
        # SPY zaten benchmark olarak Ã§ekileceÄŸi iÃ§in listeye ekleme
        if ticker == BENCHMARK_SYMBOL and include_benchmark:
            continue
        
        # Ã–nce cache analizi yap (eksik bilgi iÃ§in)
        if not force_complete:
            analysis = analyze_cache(ticker, start_date, end_date)
            if analysis.missing_days > 0 and analysis.coverage >= 0.90:
                missing_info[ticker] = {
                    "coverage": analysis.coverage,
                    "missing_days": analysis.missing_days,
                    "missing_end": analysis.missing_end.strftime('%Y-%m-%d') if analysis.missing_end else None
                }
            
        try:
            prices, source = fetch_single_stock(ticker, start_date, end_date, 
                                                force_complete=force_complete)
            all_prices.append(prices)
            data_sources[ticker] = source
        except Exception as e:
            print(f"âš ï¸ {ticker} iÃ§in veri Ã§ekilemedi: {e}")
            failed_tickers.append(ticker)
    
    # hisseleri birleÅŸtir
    if not all_prices:
        raise ValueError("HiÃ§bir hisse iÃ§in veri Ã§ekilemedi!")
    
    stock_prices = pd.concat(all_prices, axis=1)
    
    # benchmark (SPY) Ã§ek
    benchmark_prices = None
    if include_benchmark:
        try:
            benchmark_prices, bench_source = fetch_single_stock(
                BENCHMARK_SYMBOL, start_date, end_date, force_complete=force_complete
            )
            data_sources[BENCHMARK_SYMBOL] = bench_source
        except Exception as e:
            print(f"âš ï¸ Benchmark ({BENCHMARK_SYMBOL}) Ã§ekilemedi: {e}")
    
    # eksik gÃ¼nleri hizala
    stock_prices = align_and_fill(stock_prices)
    if benchmark_prices is not None:
        benchmark_prices = align_and_fill(benchmark_prices)
    
    # stock ve benchmark'i ortak tarihlere hizala
    stock_prices, benchmark_prices = align_dates(stock_prices, benchmark_prices)
    
    # meta bilgi
    meta_info = {
        "sources": data_sources,
        "failed": failed_tickers,
        "all_live": all(s == "live" for s in data_sources.values()),
        "any_cache": any(s == "cache" for s in data_sources.values()),
        "any_incremental": any(s == "cache+live" for s in data_sources.values()),
        "missing_info": missing_info  # Eksik veri bilgisi (kullanÄ±cÄ±ya gÃ¶sterilecek)
    }
    
    return stock_prices, benchmark_prices, meta_info


def align_and_fill(df: pd.DataFrame, method: str = "ffill") -> pd.DataFrame:
    """
    Eksik gÃ¼nleri doldurur ve tarihleri hizalar.
    
    Forward-fill yÃ¶ntemi:
    - Bir gÃ¼n iÃ§in veri yoksa, Ã¶nceki gÃ¼nÃ¼n deÄŸerini kullanÄ±r
    - Bu borsa tatil gÃ¼nleri iÃ§in mantÄ±klÄ± bir yaklaÅŸÄ±m
    
    Args:
        df: Fiyat DataFrame'i
        method: Doldurma yÃ¶ntemi ("ffill" = Ã¶nceki deÄŸer)
    
    Returns:
        Eksik deÄŸerleri doldurulmuÅŸ DataFrame
    """
    # once forward fill yap
    df = df.ffill()
    
    # baÅŸtaki NaN'lar iÃ§in backward fill yap (ilk gÃ¼nler veri yoksa)
    df = df.bfill()
    
    # hala NaN varsa (tÃ¼m satÄ±r boÅŸsa) o satÄ±rÄ± sil
    df = df.dropna()
    
    return df


def align_dates(
    stock_prices: pd.DataFrame,
    benchmark_prices: Optional[pd.DataFrame]
) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:
    """
    Stock ve benchmark verilerini ortak tarihlere hizalar.
    
    Bu fonksiyon, farklÄ± tarih aralÄ±klarÄ±na sahip verilerin
    doÄŸru karÅŸÄ±laÅŸtÄ±rÄ±lmasÄ±nÄ± saÄŸlar.
    
    Args:
        stock_prices: Hisse fiyatlarÄ± DataFrame'i
        benchmark_prices: Benchmark fiyatlarÄ± DataFrame'i (opsiyonel)
    
    Returns:
        Tuple: (hizalanmÄ±ÅŸ_stock, hizalanmÄ±ÅŸ_benchmark)
    """
    if benchmark_prices is None:
        return stock_prices, None
    
    # ortak tarihleri bul
    common_dates = stock_prices.index.intersection(benchmark_prices.index)
    
    if len(common_dates) == 0:
        print("âš ï¸ Stock ve benchmark arasÄ±nda ortak tarih bulunamadÄ±!")
        return stock_prices, benchmark_prices
    
    # kaybedilen gun sayisini raporla
    stock_lost = len(stock_prices) - len(common_dates)
    bench_lost = len(benchmark_prices) - len(common_dates)
    
    if stock_lost > 0 or bench_lost > 0:
        print(f"â„¹ï¸ Tarih hizalama: {len(common_dates)} ortak gÃ¼n "
              f"(stock: -{stock_lost}, benchmark: -{bench_lost})")
    
    # ortak tarihlere filtrele
    aligned_stock = stock_prices.loc[common_dates]
    aligned_benchmark = benchmark_prices.loc[common_dates]
    
    return aligned_stock, aligned_benchmark


def get_cached_tickers() -> List[str]:
    """
    Cache klasÃ¶rÃ¼ndeki mevcut ticker'larÄ± listeler.
    KullanÄ±cÄ±ya hangi verilerin hazÄ±r olduÄŸunu gÃ¶stermek iÃ§in.
    
    Returns:
        Ticker sembolleri listesi
    """
    _ensure_cache_dir()
    
    tickers = set()
    for filename in os.listdir(CACHE_DIR):
        if filename.endswith(".csv"):
            # dosya adÄ±ndan ticker'Ä± Ã§Ä±kart (ilk _ Ã¶ncesi)
            ticker = filename.split("_")[0]
            tickers.add(ticker)
    
    return sorted(list(tickers))


def clear_cache():
    """
    TÃ¼m cache dosyalarÄ±nÄ± siler.
    Dikkatli kullan - tÃ¼m Ã¶nbellek silinir!
    """
    _ensure_cache_dir()
    
    count = 0
    for filename in os.listdir(CACHE_DIR):
        if filename.endswith(".csv"):
            filepath = os.path.join(CACHE_DIR, filename)
            os.remove(filepath)
            count += 1
    
    print(f"âœ“ {count} cache dosyasÄ± silindi")


# test icin
if __name__ == "__main__":
    # basit test
    print("Veri modÃ¼lÃ¼ test ediliyor...")
    
    tickers = ["AAPL", "MSFT"]
    start = "2023-01-01"
    end = "2024-01-01"
    
    prices, benchmark = fetch_stock_data(tickers, start, end)
    
    print("\nHisse FiyatlarÄ±:")
    print(prices.head())
    
    print("\nBenchmark (SPY):")
    print(benchmark.head())
    
    print("\nâœ“ Test baÅŸarÄ±lÄ±!")
